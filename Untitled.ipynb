{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7db508b-5faf-4f69-a54b-241d47403e09",
   "metadata": {},
   "source": [
    "What can Llama tell me about my medical text descriptions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92b43892-f77a-41bc-a06a-1db11c374ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datasets\n",
    "from chat import handle_conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31254781-5277-4ce2-b913-b33714ce3c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access entire huggingface database\n",
    "#all_datasets = datasets.list_datasets()\n",
    "#len(all_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b7411a1-ebce-47d9-9900-bf6780ee165f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.load_dataset('flaviagiammarino/path-vqa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b52a5875-3478-41a4-80f3-a1c55760794c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset info: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'question', 'answer'],\n",
      "        num_rows: 19654\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['image', 'question', 'answer'],\n",
      "        num_rows: 6259\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['image', 'question', 'answer'],\n",
      "        num_rows: 6719\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(f'dataset info: {ds}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "709fc679-de4b-4007-b69a-630cf1556efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "where are liver stem cells (oval cells) located?\n",
      "what are stained here with an immunohistochemical stain for cytokeratin 7?\n",
      "what do the areas of white chalky deposits represent?\n",
      "is embolus derived from a lower-extremity deep venous thrombus lodged in a pulmonary artery branch?\n",
      "how is hyperplasia without atypia characterized?\n",
      "is normal palmar creases present?\n",
      "where is this from?\n",
      "what is present?\n",
      "what is present?\n",
      "what is present?\n"
     ]
    }
   ],
   "source": [
    "questions = []\n",
    "for train_sample_txt in ds['train']['question'][:10]:\n",
    "    print(train_sample_txt)\n",
    "    questions.append(train_sample_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47954919-1a0d-4fec-be9b-1890e19002e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['where are liver stem cells (oval cells) located?', 'what are stained here with an immunohistochemical stain for cytokeratin 7?', 'what do the areas of white chalky deposits represent?', 'is embolus derived from a lower-extremity deep venous thrombus lodged in a pulmonary artery branch?', 'how is hyperplasia without atypia characterized?', 'is normal palmar creases present?', 'where is this from?', 'what is present?', 'what is present?', 'what is present?']\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffc20b7f-be17-4cc4-8cf9-60a93fbddea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chat import handle_conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508dde83-20e5-4d74-bee6-2c252eb738ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to LLama neural-chat ChatBot. Type 'exit' to quit\n",
      "questions: ['where are liver stem cells (oval cells) located?', 'what are stained here with an immunohistochemical stain for cytokeratin 7?', 'what do the areas of white chalky deposits represent?', 'is embolus derived from a lower-extremity deep venous thrombus lodged in a pulmonary artery branch?', 'how is hyperplasia without atypia characterized?', 'is normal palmar creases present?', 'where is this from?', 'what is present?', 'what is present?', 'what is present?']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  what are this questions about\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI chatbot:  These questions cover a variety of topics, but they all seem to have a medical or biology focus. The conversation history includes discussions on liver stem cells (oval cells), staining techniques in pathology, chalky deposits, thrombus and embolism, hyperplasia without atypia, normal palmar creases, the origin of something, describing what is present within context, and a more general inquiry about context or origin.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  what are the most frequent keywords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI chatbot:  The most frequent keywords from this conversation history appear to be 'liver stem cells', 'oval cells', 'immunohistochemical stain', 'cytokeratin 7', 'white chalky deposits', 'embolus', 'deep venous thrombus', 'pulmonary artery branch', 'hyperplasia without atypia', 'normal palmar creases', and 'present'. These keywords provide insight into the topics discussed, which are predominantly related to medical or biological concepts.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  can you give me idea of a project where I can use this dataset and AI. Each of this questions pair with sepcific image that they describe, and there are almost 20000 train samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI chatbot:  To create an AI-powered medical or biology application using your given dataset as input, you could develop a visual question answering system. This would require training a deep learning model on the dataset, which includes both questions and corresponding images that are linked to specific concepts such as pathology, anatomy, or biological systems.\n",
      "\n",
      "To start with this project, follow these steps:\n",
      "\n",
      "1. Organize the 20,000 train samples into labeled subsets based on their associated questions and topics. This organization will help in training separate models for each category of questions.\n",
      "\n",
      "2. Prepare image datasets to train a classification model that can identify the presence or absence of the concepts mentioned in your questions (e.g., oval cells, white chalky deposits). Train this model using a deep learning framework like TensorFlow or Keras and a pre-trained ResNet50 or InceptionV3 model as a starting point for feature extraction.\n",
      "\n",
      "3. Develop a natural language processing system to process questions related to the images, possibly employing BERT, RoBERTa, or T5 models for this task. Train these models on a corpus of medical and biological texts. This part will ensure that your AI can understand complex biology-specific language accurately.\n",
      "\n",
      "4. Combine both parts using an API to create a system capable of answering questions about the images in the dataset. For example, if you input \"What are white chalky deposits?\" while showing the image, the system should be able to recognize and respond with the relevant context from your prepared training data.\n",
      "\n",
      "5. Continuously improve this system by adding more samples to your training datasets and updating both the visual classification model and NLP models as needed. This way, you will create a robust medical or biology-focused AI system for various use cases.\n"
     ]
    }
   ],
   "source": [
    "handle_conversation(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e58f09-8a23-411c-a3b5-80a8c5350dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38329c0d-f590-48ef-abb4-609f2b471177",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b80ed420-1eeb-46ab-930d-e5f7edf47e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"REPLICATE_API_TOKEN\"] = 'r8_IG3urlz3pCOTmJ4Cj5cufVBcpkbI9X31FohnJ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96756a63-7c37-4882-8c9d-26f41c64cb2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ReplicateError",
     "evalue": "ReplicateError Details:\ntitle: Free time limit reached\nstatus: 402\ndetail: You have reached the free time limit. To continue using Replicate, set up billing at https://replicate.com/account/billing#billing.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mReplicateError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# The meta/llama-2-70b-chat model can stream output as it's running.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m replicate\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta/llama-2-70b-chat\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI have QA dataset and here is a subsample of my questions, consider content between \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as a separate question \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(questions)\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthis is a ground truth dataset, tell me what is it about in one sentence\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m     },\n\u001b[1;32m      8\u001b[0m ):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(event), end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/huggingface/lib/python3.12/site-packages/replicate/stream.py:191\u001b[0m, in \u001b[0;36mstream\u001b[0;34m(client, ref, input, **params)\u001b[0m\n\u001b[1;32m    187\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mpredictions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m    188\u001b[0m         version\u001b[38;5;241m=\u001b[39m(version \u001b[38;5;129;01mor\u001b[39;00m version_id), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m {}, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    189\u001b[0m     )\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m owner \u001b[38;5;129;01mand\u001b[39;00m name:\n\u001b[0;32m--> 191\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mpredictions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m    192\u001b[0m         model\u001b[38;5;241m=\u001b[39m(owner, name), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m {}, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    193\u001b[0m     )\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected model, version, or reference in the format owner/name or owner/name:version\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    197\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/huggingface/lib/python3.12/site-packages/replicate/model.py:396\u001b[0m, in \u001b[0;36mModelsPredictions.create\u001b[0;34m(self, model, input, **params)\u001b[0m\n\u001b[1;32m    393\u001b[0m url \u001b[38;5;241m=\u001b[39m _create_prediction_url_from_model(model)\n\u001b[1;32m    394\u001b[0m body \u001b[38;5;241m=\u001b[39m _create_prediction_body(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 396\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    398\u001b[0m     url,\n\u001b[1;32m    399\u001b[0m     json\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    400\u001b[0m )\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _json_to_prediction(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client, resp\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/huggingface/lib/python3.12/site-packages/replicate/client.py:88\u001b[0m, in \u001b[0;36mClient._request\u001b[0;34m(self, method, path, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, method: \u001b[38;5;28mstr\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m httpx\u001b[38;5;241m.\u001b[39mResponse:\n\u001b[1;32m     87\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mrequest(method, path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 88\u001b[0m     _raise_for_status(resp)\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/huggingface/lib/python3.12/site-packages/replicate/client.py:375\u001b[0m, in \u001b[0;36m_raise_for_status\u001b[0;34m(resp)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_for_status\u001b[39m(resp: httpx\u001b[38;5;241m.\u001b[39mResponse) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m400\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m resp\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m600\u001b[39m:\n\u001b[0;32m--> 375\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ReplicateError\u001b[38;5;241m.\u001b[39mfrom_response(resp)\n",
      "\u001b[0;31mReplicateError\u001b[0m: ReplicateError Details:\ntitle: Free time limit reached\nstatus: 402\ndetail: You have reached the free time limit. To continue using Replicate, set up billing at https://replicate.com/account/billing#billing."
     ]
    }
   ],
   "source": [
    "# The meta/llama-2-70b-chat model can stream output as it's running.\n",
    "for event in replicate.stream(\n",
    "    \"meta/llama-2-70b-chat\",\n",
    "    input={\n",
    "        \"prompt\": \"I have QA dataset and here is a subsample of my questions, consider content between ' ' as a separate question \" + str(questions)\n",
    "        + \"this is a ground truth dataset, tell me what is it about in one sentence\"\n",
    "    },\n",
    "):\n",
    "    print(str(event), end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61b8238-19af-4c8f-a57f-ed9761d7a675",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
